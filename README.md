# Legacy Modernization Agents to migrate COBOL to Java or C# developed with the Semantic Kernel Process Function

This open source migration framework was developed to demonstrate AI Agents capabilities for converting legacy code like COBOL to Java or C# .NET. Each Agent has a persona that can be edited depending on the desired outcome.
The migration is using Semantic Kernel Process Function where it does analysis of the COBOL code and it's dependencies. This information is then used to convert to either Java Quarkus or C# .NET (user's choice).

## Acknowledgements of collaboration
This project is a collaboration between Microsoft's Global Black Belt team and [Bankdata](https://www.bankdata.dk/). If you want to learn more about the collaboration and background of this project, have a look at [this](https://aka.ms/cobol-blog) and [this](https://www.bankdata.dk/about/news/microsoft-and-bankdata-launch-open-source-ai-framework-for-modernizing-legacy-systems) blog post.

## Call-to-Action
We are looking for real COBOL code to further improve this framework. If you want to actively collaborate, please reach out to us by opening an issue in this repository. - Gustav Kaleta & Julia Kordick

# Want to see the framework in action?
Have a look at the talk Julia did at the WeAreDevelopers World Congress 2025: https://www.youtube.com/watch?v=62OI_y-KRlw

## Table of Contents
- [Quick Start](#-quick-start) - Prerequisites, Dev Container, Neo4j, Demo
- [Features](#-features) - Core capabilities and functionality
- [Architecture](#-architecture) - Hybrid databases, system design, portal UI
- [CLI Reference](#-cli-commands-reference) - Doctor.sh and .NET commands
- [Step-by-Step Guide](#step-by-step-guide) - Configuration to deployment
- [How It Works](#how-it-works---architecture--flow) - Technical details
- [Azure Deployment](#ï¸-azure-container-apps-deployment) - Deploy to Azure with azd
- [Known Issues & Ideas](#known-issues) - Troubleshooting and roadmap

## ğŸš€ Quick Start

### Prerequisites Checklist

- [ ] Docker Desktop installed and running
- [ ] .NET 9.0 SDK installed (`dotnet --version` shows 9.0.x) - [Download](https://dotnet.microsoft.com/download/dotnet/9.0)
- [ ] Azure OpenAI credentials ready (endpoint + API key) - GPT-5 and codex recommended
- [ ] 8GB RAM minimum, 16GB recommended
- [ ] Modern browser (Chrome, Edge, Firefox, Safari)

> **Note:** Hybrid database architecture: SQLite (metadata) + Neo4j (dependency graphs)

### âš¡ Fast Track Setup (Dev Container)

**Best for:** Team collaboration, consistent environment

```bash
# 1. Clone repository
git clone https://github.com/Azure-Samples/Legacy-Modernization-Agents.git
cd Legacy-Modernization-Agents

# 2. Open in VS Code
code .

# 3. When prompted, click "Reopen in Container"
# Wait 3-5 minutes for first-time setup

# 4. Configure Azure OpenAI (in container terminal)
cp Config/ai-config.local.env.example Config/ai-config.local.env
nano Config/ai-config.local.env
# Edit: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME

# 5. Run demo
./helper-scripts/demo.sh
# OR if in devcontainer, simply use:
demo
```

âœ… **Done!** Portal opens at http://localhost:5028

<img width="1715" height="963" alt="Portal experience with metadata and graph data fronted by MCP" src="gifdemowithgraphandreportign.gif" />

**Fully automated environment** with .NET 9, Java 17, Neo4j, SQLite, Azure CLI, and pre-configured VS Code extensions.

**Included:** C# Dev Kit, Java Pack, Quarkus, Semantic Kernel, Neo4j, SQLite extensions  
**Aliases:** `demo`, `migration-run`, `portal-start`, `neo4j-status`

---

### ğŸ”§ Manual Setup (Local Development)

**Best for:** Experienced developers, custom configurations

#### Step 1: Install Prerequisites

```bash
# Check .NET version
dotnet --version
# Should show: 9.0.x

# If not, install from: https://dotnet.microsoft.com/download/dotnet/9.0

# Check Docker
docker --version
docker ps
```

#### Step 2: Start Neo4j

```bash
# Option A: Docker Compose (recommended)
docker-compose up -d neo4j

# Option B: Manual Docker
docker run -d \
  --name cobol-migration-neo4j \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/cobol-migration-2025 \
  neo4j:5.15.0

# Verify
docker ps | grep neo4j
# Wait 30 seconds for Neo4j to be ready
```

#### Step 3: Configure Azure OpenAI

```bash
# Copy template
cp Config/ai-config.local.env.example Config/ai-config.local.env

# Edit with your credentials
nano Config/ai-config.local.env
```

**Required values:**
```bash
AZURE_OPENAI_ENDPOINT="https://YOUR-RESOURCE.openai.azure.com/"
AZURE_OPENAI_API_KEY="your-32-character-key-here"
AZURE_OPENAI_DEPLOYMENT_NAME="gpt-5-mini-2"
```

**Find your values:**
- Endpoint: Azure Portal â†’ OpenAI Resource â†’ Keys and Endpoint â†’ Endpoint
- API Key: Azure Portal â†’ OpenAI Resource â†’ Keys and Endpoint â†’ Key 1
- Deployment: Your deployment name (e.g., "gpt-5-mini-2" or "gpt-4o")

#### Step 4: Build Project

```bash
# Restore dependencies
dotnet restore

# Build solution
dotnet build

# Verify zero warnings
# Output should show: "0 Warning(s), 0 Error(s)"
```

#### Step 5: Run Demo

```bash
# One command to start everything
./helper-scripts/demo.sh
# OR if in devcontainer:
demo

# Or manual steps:
# 1. Ensure Neo4j is running
# 2. cd McpChatWeb
# 3. dotnet run
# 4. Open http://localhost:5028
```

---

### ğŸ¬ First Run Experience

After running `./helper-scripts/demo.sh`, you'll see:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   COBOL Migration Portal - Demo Mode                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Checking prerequisites...
âœ… Docker installed
âœ… .NET 9.0 SDK found
âœ… Neo4j accessible

ğŸ“Š Starting Neo4j...
âœ… Neo4j is running

ğŸ’¾ Checking database...
âœ… Database found with Run 43

ğŸš€ Starting web portal...
â³ Waiting for portal to start âœ…

ğŸŒ Access your demo:
   Portal:        http://localhost:5028
   Neo4j Browser: http://localhost:7474
```

### ğŸŒ Accessing the Portal

**In VS Code Dev Container:**
1. Look for the "PORTS" tab in the bottom panel (next to Terminal)
2. Find port `5028` in the list
3. Click the **globe icon (ğŸŒ)** to open in browser
4. Or run: `./helper-scripts/open-portal.sh`

**In Local Environment:**
- Browser will auto-open to http://localhost:5028
- Or manually visit: http://localhost:5028

### ğŸ“Š Portal Features
- Three-panel dashboard
- AI-powered chat with Azure OpenAI
- Interactive dependency graph
- Multi-run support
- Real-time file analysis

---

### ğŸ§ª Test Your Setup

#### 1. Verify Portal Loads

**Expected:**
- âœ… Three panels visible (resources | chat | graph)
- âœ… Graph shows "Dependency Graph | Run 43"
- âœ… Resources list shows 8 MCP resources
- âœ… Suggestion chips visible in chat

**If not working:**
```bash
# Check portal logs
# Look for "Now listening on: http://localhost:5028"
```

#### 2. Test Chat

**Try this query:**
```
"Hello, what can you help me with?"
```

**Expected response:**
- âœ… AI responds with migration capabilities
- âœ… Response appears within 3-5 seconds
- âœ… No error messages

**If error:**
```bash
# Check Azure OpenAI config
./doctor.sh doctor
```

#### 3. Test File Analysis

**Try this query:**
```
"What functions are in BDSDA23.cbl?"
```

**Expected response:**
- âœ… Program purpose description
- âœ… List of functions/paragraphs
- âœ… Variables with PIC clauses
- âœ… Copybooks referenced
- âœ… Data source URI shown

#### 4. Test Multi-Run

**Try this query:**
```
"Show me run 42"
```

**Expected:**
- âœ… Response shows Run 42 data
- âœ… Graph updates automatically
- âœ… Graph header shows "Run 42"
- âœ… Response includes SQLite and Neo4j data

#### 5. Test Graph

**Actions:**
- Click **"Full Graph"** filter
- Zoom in with mouse wheel
- Click on a blue node (program)
- Click on a red node (copybook)
- Hover over edges (relationships)

**Expected:**
- âœ… Graph displays 49 nodes + 64 edges
- âœ… Smooth zooming and panning
- âœ… Tooltips appear on hover
- âœ… Node selection highlights

---

### ğŸ“ First Tasks to Try

#### Task 1: Explore Dependencies (2 minutes)

```
1. Click "ğŸ”„ Circular Dependencies" chip
2. Read AI response about cycles
3. Click "Programs Only" filter
4. Observe only blue nodes shown
5. Click "Full Graph" to restore
```

#### Task 2: Analyze a File (3 minutes)

```
1. Type: "What does RENI033.cpy contain?"
2. Read the analysis response
3. Note the variables and their PIC clauses
4. Click on the file node in graph
5. Explore relationships visually
```

#### Task 3: Compare Runs (3 minutes)

```
1. Type: "Show me run 43"
2. Note the statistics (102 files, etc.)
3. Type: "Now show me run 42"
4. Watch graph update automatically
5. Compare the two runs
```

#### Task 4: Access Raw Data (5 minutes)

```
1. Click "ğŸ“– Data Retrieval Guide"
2. Copy SQLite query example
3. Open terminal
4. Run: sqlite3 Data/migration.db
5. Paste query and explore
```

---

### â“ Common Issues & Solutions

#### Issue: "NETSDK1045: The current .NET SDK does not support targeting .NET 9.0"

**Solution:**
```bash
# Install .NET 9.0 SDK
https://dotnet.microsoft.com/download/dotnet/9.0

# Verify
dotnet --version
# Should show: 9.0.x
```

#### Issue: "Neo4j connection refused"

**Solution:**
```bash
# Check if running
docker ps | grep neo4j

# If not running, start it
docker-compose up -d neo4j

# Wait 30 seconds, then verify
curl http://localhost:7474
```

#### Issue: "Azure OpenAI API error"

**Solution:**
```bash
# Check configuration
./doctor.sh doctor

# Verify endpoint ends with /
# Verify API key is 32 characters
# Verify deployment name matches your Azure OpenAI deployment (e.g., "gpt-5-mini-2")

# Test connection
./doctor.sh test
```

#### Issue: "Portal shows no data"

**Solution:**
```bash
# Check database exists
ls -lh Data/migration.db

# If missing, run a migration first
./doctor.sh run

# Or use demo mode (uses existing data)
./helper-scripts/demo.sh
# OR if in devcontainer:
demo
```

#### Issue: "Graph not displaying"

**Solution:**
1. Open browser DevTools (F12)
2. Check Console tab for JavaScript errors
3. Verify `/api/graph` endpoint returns data:
   ```bash
   curl http://localhost:5028/api/graph | jq '.nodes | length'
   # Should show: 49
   ```
4. Hard refresh browser (Ctrl+Shift+R or Cmd+Shift+R)

---

### ğŸ‰ Success Criteria

You're all set when:

- âœ… Portal loads at http://localhost:5028
- âœ… Chat responds to queries
- âœ… Graph displays nodes and edges
- âœ… File analysis returns detailed data
- âœ… Multi-run queries work
- âœ… Zero build warnings
- âœ… Neo4j accessible at http://localhost:7474

**Time to start migrating COBOL! ğŸš€**

---

#### Screenshots
<img width="1715" height="963" alt="Portal with metadata and graph visualization" src="https://github.com/user-attachments/assets/c1faca51-dc21-41cf-9a51-70da5a3c8255" />
<img width="802" height="855" alt="MCP fronting Neo4j with Azure OpenAI" src="https://github.com/user-attachments/assets/2b93d018-0d54-479a-a090-2d6eb40f391e" />

## âœ¨ Features

### 1. ğŸ”„ Dual Language Support (Java & C#)

**Choose Your Target Language:**
- Interactive selection during migration: Java Quarkus or C# .NET
- Separate converter agents optimized for each language
- Unified `output/` folder for generated code
- Agent personas customized for Java/Quarkus or C#/.NET expertise

### 2. ğŸ” Reverse Engineering

**Extract Business Logic Without Full Migration:**
- Standalone `reverse-engineer` command
- Generates human-readable documentation from COBOL
- Produces business features, user stories, and domain models
- Glossary support (`Data/glossary.json`) translates technical terms
- Output: Comprehensive markdown with business rules and architecture

### 3. ğŸ—„ï¸ Hybrid Database Architecture

**SQLite + Neo4j for Comprehensive Data Storage:**
- **SQLite**: Migration metadata, runs, file analysis, structured data
- **Neo4j**: Dependency graphs, relationships, visual analytics
- Real-time LOC calculation (excludes comments and blank lines)
- Multi-run queries: Switch context between historical migrations
- Full-text search capabilities

### 4. ğŸ“Š Enhanced Dependency Tracking

**Comprehensive COBOL Statement Detection:**
- **CALL** - Program invocations with line numbers
- **COPY** - Copybook inclusions
- **PERFORM** - Procedure calls and loops
- **EXEC SQL** - Embedded SQL statements
- **READ/WRITE** - File I/O operations
- **OPEN/CLOSE** - File handling

**Visual Differentiation:**
- Color-coded edges in dependency graph (CALL=green, COPY=blue, etc.)
- Edge labels show type and line numbers (e.g., "CALL (L42)")
- Tooltips display full context from source code
- Filterable by edge type with checkboxes

**Example:**
```cobol
Line 42: CALL 'FORMAT-BALANCE' USING WS-AMOUNT
```
Appears as: `CUSTOMER-DISPLAY.cbl â†’ FORMAT-BALANCE.cbl [CALL (L42)]`

### 5. ğŸ’¬ File Content Analysis

Ask natural language questions about COBOL file contents directly in the chat interface:

**Example Queries:**
```plaintext
"What functions are in BDSDA23.cbl?"
"What methods are used in RGNB649.cbl?"
"What does the copybook RENI033.cpy contain code wise?"
"Show me the variables in BDSIW13.cbl"
"What paragraphs are in AGSFZ01.cbl?"
```

**What You Get:**
- âœ… **Program Purpose**: High-level description of what the file does
- âœ… **All Functions/Paragraphs**: Complete list with descriptions and logic summaries
- âœ… **Variables**: Top 15 variables with PIC clauses, types, levels, usage
- âœ… **Copybooks Referenced**: All COPY statements and dependencies
- âœ… **Data Source**: MCP resource URI with API endpoint reference

**How It Works:**
1. Chat endpoint detects file-related queries using regex pattern
2. Queries MCP resource: `insights://runs/{runId}/analyses/{fileName}`
3. Parses `rawAnalysisData` JSON field for detailed structure
4. Extracts from nested arrays: `paragraphs-and-sections-summary`, `variables`, `copybooksReferenced`
5. Falls back to SQLite direct query if MCP unavailable

**Example Response:**
```markdown
ğŸ“„ Analysis for BDSDA23.cbl (Run 43)

**Purpose:**
Batch data synchronization agent for daily transaction processing

**Functions/Paragraphs (23):**
- **`MAIN-PROCESS`**: Main entry point, orchestrates batch workflow
- **`VALIDATE-INPUT`**: Validates input file records for completeness
- **`PROCESS-TRANSACTIONS`**: Iterates through transactions and updates database
...

**Variables (15):**
- `WS-RECORD-COUNT` PIC 9(8) (numeric)
- `WS-TRANSACTION-DATE` PIC X(10) (alphanumeric)
- `WS-ERROR-FLAG` PIC X (boolean)
... and 8 more

**Copybooks Referenced (5):**
- RENI033.cpy
- BDSCOPY1.cpy
- COMMON.cpy

**Data Source:** MCP Resource URI: `insights://runs/43/analyses/BDSDA23.cbl`
**API:** `GET /api/file-analysis/BDSDA23.cbl?runId=43`
```

### 6. ğŸ”„ Multi-Run Query Support

Query any historical run: "Show me run 42". Automatically queries both SQLite and Neo4j, updates graph visualization, and labels data sources. Perfect for comparing different migration attempts or analyzing specific runs.

### 7. ğŸ“ˆ Dynamic Graph Updates

Graph auto-updates when querying different runs. Frontend detects `runId` in response and refreshes visualization without manual intervention. Interactive vis-network graph with zoom, pan, and filtering capabilities.

### 8. ğŸ“„ Migration Report Generation

**Generate Comprehensive Reports:**
- **Via Portal**: Click "ğŸ“„ Generate Report" on any migration run
- **Via CLI**: Prompted after each `./doctor.sh run` completion
- **Via API**: `GET /api/runs/{runId}/report`

**Report Contents:**
- Migration summary (file counts, target language)
- Dependency breakdown by type (CALL, COPY, PERFORM, etc.)
- Complete file inventory with line counts
- Detailed dependency relationships table
- Line numbers and context for each dependency

**Report Formats:**
- View rendered in portal with Markdown formatting
- Download as `.md` file for documentation
- Auto-saved to `output/migration_report_run_{runId}.md`

**Example Report Structure:**
```markdown
# COBOL Migration Report - Run 6

## ğŸ“Š Migration Summary
- Total COBOL Files: 5
- Programs (.cbl): 3
- Copybooks (.cpy): 2
- Total Dependencies: 8
  - CALL: 2
  - COPY: 3
  - PERFORM: 2
  - READ: 1

## ğŸ“ File Inventory
| File Name | Path | Lines |
|-----------|------|-------|
| CUSTOMER-INQUIRY.cbl | source/ | 156 |
...

## ğŸ”— Dependency Relationships
| Source | Target | Type | Line | Context |
|--------|--------|------|------|----------|
| CUSTOMER-INQUIRY.cbl | CUSTOMER-DISPLAY.cbl | CALL | 42 | CALL 'CUSTOMER-DISPLAY' |
...
```

### 9. ğŸ¨ Mermaid Diagram Support

**Interactive Architecture Documentation:**
- All Mermaid flowcharts, sequence diagrams, and visualizations render automatically
- Dark theme matching portal design
- Zoomable and pan-able diagrams
- Syntax highlighting for code blocks
- Supported diagram types: Flowcharts, Sequence, Class, ER diagrams
- Integrated into "ğŸ“„ Architecture Documentation" portal view

### 10. ğŸŒ Interactive Web Portal

**Comprehensive Dashboard at http://localhost:5028:**
- Three-panel layout: Resources | Chat | Graph Visualization
- AI-powered chat with MCP-backed responses
- Run selector dropdown with historical runs
- Dark theme with rich colors and modern UI
- Real-time updates and interactive controls
- Example queries section for quick analysis
- Data retrieval guide modal

### 11. ğŸ”Œ MCP (Model Context Protocol) Integration

**9 MCP Resources per Migration Run:**
- Run summary, file lists, dependency graphs
- Circular dependency analysis, critical files
- Individual COBOL and generated code files
- Per-file dependencies and detailed analyses
- REST API endpoints for programmatic access

### 12. ğŸš€ DevContainer Auto-Start

**Fully Automated Development Environment:**
- Services launch automatically on container restart
- Portal auto-starts at http://localhost:5028
- Neo4j with health checks and auto-recovery
- Pre-configured VS Code extensions
- Bash aliases for common commands
- Locked ports prevent configuration drift

---

## ğŸ—ï¸ Architecture

**Supported Diagram Types:**
- Flowcharts (system architecture)
- Sequence diagrams (process flows)
- Class diagrams (data models)
- ER diagrams (database schemas)

**Example:**
Markdown with `mermaid` code blocks automatically renders as interactive SVG diagrams in the browser.

**Technologies:**
- **Mermaid.js 10.x** - Diagram rendering engine
- **Marked.js 11.x** - Markdown parser
- Dark theme with custom color scheme matching portal

---

### 4. ğŸ“š Data Retrieval Guide

Access comprehensive data access documentation directly in the portal via the **"ğŸ“– Data Retrieval Guide"** button.

**What's Included:**
- ğŸ—„ï¸ **SQLite Instructions**: 
  - Database location and schema
  - 5 example queries (runs, files, analyses, Java code, dependencies)
  - Tool recommendations (sqlite3 CLI, DB Browser, VS Code extension)
  
- ğŸ”— **Neo4j Instructions**:
  - Connection details (bolt://localhost:7687)
  - Credentials (neo4j / cobol-migration-2025)
  - 5 Cypher queries (runs, files, dependencies, circular deps, critical files)
  - Tool recommendations (Neo4j Browser, Desktop, cypher-shell)

- ğŸ¯ **MCP API Instructions**:
  - All available MCP resource URIs
  - REST API endpoints (/api/resources, /api/chat, /api/graph)
  - Example API calls with curl commands

- ğŸ“‹ **Copy-Paste Examples**:
  - Three complete workflows (SQLite, Neo4j, API)
  - Step-by-step commands ready to use
  - No configuration needed

**API Endpoint:**
```bash
GET /api/data-retrieval-guide
```

**Modal Features:**
- Dark theme matching portal design
- Syntax highlighting for code blocks
- Organized in collapsible sections
- Close with X button or click outside

---

## ï¿½ğŸ—ï¸ Complete Architecture

### ğŸ—„ï¸ Hybrid Database Architecture

This project uses a **dual-database approach** for optimal performance and functionality:

```mermaid
flowchart TB
    subgraph INPUT["ğŸ“ Input Layer"]
        COBOL["COBOL Source Files<br/>(.cbl, .cpy)"]
    end
    
    subgraph PROCESS["ğŸ”„ Migration Process"]
        ANALYZER["ğŸ” CobolAnalyzerAgent<br/>Structure Analysis"]
        CONVERTER["â˜•/C# CodeConverterAgent<br/>Java Quarkus or C# .NET Translation<br/>(User's Choice)"]
        MAPPER["ğŸ—ºï¸ DependencyMapperAgent<br/>Relationship Analysis"]
    end
    
    subgraph STORAGE["ğŸ’¾ Hybrid Storage Layer"]
        SQLITE[("ğŸ“Š SQLite Database<br/>Data/migration.db<br/><br/>â€¢ Run metadata<br/>â€¢ File content<br/>â€¢ AI analyses<br/>â€¢ Java/C# code<br/>â€¢ Metrics")]
        NEO4J[("ğŸ”— Neo4j Graph DB<br/>bolt://localhost:7687<br/><br/>â€¢ Dependencies<br/>â€¢ Relationships<br/>â€¢ Graph data<br/>â€¢ Visualizations")]
    end
    
    subgraph REPO["ğŸ”€ Repository Layer"]
        HYBRID["HybridMigrationRepository<br/>Coordinates both databases"]
    end
    
    subgraph ACCESS["ğŸŒ Access Layer"]
        MCP["ğŸ¯ MCP Server<br/>JSON-RPC API"]
        PORTAL["ğŸ–¥ï¸ Web Portal<br/>localhost:5250"]
        BROWSER["ğŸ” Neo4j Browser<br/>localhost:7474"]
    end
    
    subgraph OUTPUT["ğŸ“¦ Output"]
        JAVA["â˜• Java Quarkus Code<br/>OR<br/>C# .NET Code"]
        REPORTS["ğŸ“Š Reports & Logs"]
    end
    
    COBOL --> PROCESS
    PROCESS --> ANALYZER
    PROCESS --> CONVERTER
    PROCESS --> MAPPER
    
    ANALYZER --> HYBRID
    CONVERTER --> HYBRID
    MAPPER --> HYBRID
    
    HYBRID --> SQLITE
    HYBRID --> NEO4J
    
    SQLITE --> MCP
    NEO4J --> MCP
    NEO4J --> BROWSER
    
    MCP --> PORTAL
    
    PROCESS --> JAVA
    PROCESS --> REPORTS
    
    classDef inputStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    classDef processStyle fill:#f1f8e9,stroke:#689f38,stroke-width:3px,color:#000
    classDef storageStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px,color:#000
    classDef accessStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000
    classDef outputStyle fill:#e8f5e9,stroke:#388e3c,stroke-width:3px,color:#000
    
    class COBOL inputStyle
    class ANALYZER,CONVERTER,MAPPER processStyle
    class SQLITE,NEO4J,HYBRID storageStyle
    class MCP,PORTAL,BROWSER accessStyle
    class JAVA,REPORTS outputStyle
```

#### ğŸ“Š SQLite Database Stores:
- âœ… **Migration run metadata** (ID, status, timestamps, statistics)
- âœ… **COBOL file content** (original source code)
- âœ… **AI-generated analyses** (structured insights)
- âœ… **Generated Java code** (converted output)
- âœ… **Historical data** (all previous runs)
- âœ… **Metrics and performance** (tokens, costs, timings)

**Location**: `Data/migration.db` (configurable via `MIGRATION_DB_PATH`)

#### ğŸ”— Neo4j Graph Database Stores:
- âœ… **File-to-file dependencies** (program â†’ copybook)
- âœ… **Call relationships** (program â†’ program)
- âœ… **Transitive dependencies** (indirect relationships)
- âœ… **Graph visualization data** (for UI rendering)
- âœ… **Impact analysis** (what files are affected by changes)

**Connection**: `bolt://localhost:7687` (configured in `appsettings.json`)

#### ğŸ”€ Why Both Databases?

| Aspect | SQLite | Neo4j |
|--------|--------|-------|
| **Purpose** | Document storage | Relationship mapping |
| **Strength** | Fast queries, simple setup | Graph traversal, visualization |
| **Data Type** | Flat/relational data | Connected graph data |
| **Use Case** | "What's in this file?" | "What depends on this file?" |
| **Query Style** | SQL SELECT statements | Cypher graph queries |

**Together they provide**: Fast metadata access + Powerful relationship insights ğŸš€

### ğŸ–¼ï¸ Three-Panel Portal UI

The web portal at `localhost:5250` features a modern three-panel layout:

```mermaid
graph TB
    subgraph PORTAL["ğŸŒ Web Portal Layout (localhost:5250)"]
        direction LR
        subgraph LEFT["ğŸ“‹ Left Panel<br/>(300px)"]
            RESOURCES["MCP Resources<br/>â€¢ Migration Runs<br/>â€¢ COBOL Files<br/>â€¢ Dependencies<br/>â€¢ Java Files<br/>â€¢ Graph Queries"]
        end
        
        subgraph CENTER["ğŸ’¬ Center Panel<br/>(Flexible)"]
            CHAT["Chat Interface<br/>â€¢ Ask questions<br/>â€¢ AI responses<br/>â€¢ Conversation history"]
            CHIPS["6 Suggestion Chips<br/>â€¢ Circular Dependencies<br/>â€¢ Critical Files<br/>â€¢ Impact Analysis<br/>â€¢ File Relationships<br/>â€¢ Dependency Summary<br/>â€¢ Conversion Stats"]
        end
        
        subgraph RIGHT["ğŸ“Š Right Panel<br/>(500px)"]
            GRAPH["Dependency Graph<br/>â€¢ Interactive visualization<br/>â€¢ Zoom & pan<br/>â€¢ Click for details<br/>â€¢ Query filters<br/>â€¢ Layout options"]
        end
    end
    
    LEFT -.-> CENTER
    CENTER -.-> RIGHT
    
    classDef leftStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef centerStyle fill:#f1f8e9,stroke:#689f38,stroke-width:2px,color:#000
    classDef rightStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    
    class LEFT,RESOURCES leftStyle
    class CENTER,CHAT,CHIPS centerStyle
    class RIGHT,GRAPH rightStyle
```

**Portal Features:** 
- âœ… Dark theme with modern UI
- âœ… Three-panel layout (resources/chat/graph)
- âœ… AI-powered chat interface
- âœ… Suggestion chips for common queries
- âœ… Interactive dependency graph (zoom/pan/filter)
- âœ… Multi-run queries and comparisons
- âœ… File content analysis with line counts
- âœ… Comprehensive data retrieval guide
- âœ… **NEW:** Enhanced dependency tracking (CALL, COPY, PERFORM, EXEC, READ, WRITE, OPEN, CLOSE)
- âœ… **NEW:** Migration report generation per run
- âœ… **NEW:** Mermaid diagram rendering in documentation
- âœ… **NEW:** Collapsible filter sections for cleaner UI
- âœ… **NEW:** Edge type filtering with color-coded visualization
- âœ… **NEW:** Line number context for all dependencies

### ğŸ”„ Agent Flowchart

```mermaid
flowchart TD
  CLI[["CLI / doctor.sh\n- Loads AI config, target language, concurrency\n- Orchestrates reverse engineering + conversion"]]
  REVERSE["ReverseEngineeringProcess\nInputs: COBOL folder, glossary\nOutputs: analyses + business logic\nSteps (parallelizable):\n1. CobolAnalyzerAgent\n2. BusinessLogicExtractorAgent\n3. Persist to SQLite/Neo4j"]
  MIGRATION["MigrationProcess\nInputs: ReverseEngineeringResult + settings\nOutputs: Java/C# code, dependency graphs, reports\nSteps:\n1. CobolAnalyzerAgent (reuse/refresh)\n2. DependencyMapperAgent\n3. CodeConverterAgent (Java or C#)\n4. Persist run data\n5. Optional report"]
  STORAGE["Repositories & Storage\n- SqliteMigrationRepository\n- Neo4jMigrationRepository\n- HybridMigrationRepository"]
  ACCESS["Access / Presentation\n- MCP Server + McpProcessClient\n- McpChatWeb portal\n- External MCP clients (Cursor, Claude, etc.)"]

  CLI --> REVERSE --> MIGRATION --> STORAGE --> ACCESS
```

### ğŸ”€ Agent Responsibilities & Interactions

#### Advanced Sequence Flow (Mermaid)

```mermaid
sequenceDiagram
  participant User as ğŸ§‘ User / doctor.sh
  participant CLI as CLI Runner
  participant RE as ReverseEngineeringProcess
  participant Analyzer as CobolAnalyzerAgent
  participant BizLogic as BusinessLogicExtractorAgent
  participant Migration as MigrationProcess
  participant DepMap as DependencyMapperAgent
  participant Converter as CodeConverterAgent (Java/C#)
  participant Repo as HybridMigrationRepository
  participant Portal as MCP Server & McpChatWeb

  User->>CLI: select target language, concurrency flags
  CLI->>RE: start reverse engineering
  RE->>Analyzer: analyze COBOL files (parallel up to max-parallel)
  Analyzer-->>RE: CobolAnalysis[]
  RE->>BizLogic: extract business logic summaries
  BizLogic-->>RE: BusinessLogic[]
  RE->>Repo: persist analyses + documentation
  RE-->>CLI: ReverseEngineeringResult
  CLI->>Migration: start migration run with latest analyses
  Migration->>Analyzer: reuse or refresh CobolAnalysis
  Migration->>DepMap: build dependency graph (CALL/COPY/...)
  DepMap-->>Migration: DependencyMap
  Migration->>Converter: convert to Java/C# (AI-limited concurrency)
  Converter-->>Migration: CodeFile artifacts
  Migration->>Repo: persist run metadata, graph edges, code files
  Repo-->>Portal: expose MCP resources + REST APIs
  Portal-->>User: portal UI (chat, graph, reports)
```

#### CobolAnalyzerAgent
- **Purpose:** Deep structural analysis of COBOL files (divisions, paragraphs, copybooks, metrics).
- **Inputs:** COBOL text from `FileHelper` or cached content.
- **Outputs:** `CobolAnalysis` objects consumed by:
  - `ReverseEngineeringProcess` (for documentation & glossary mapping)
  - `DependencyMapperAgent` (seed data for relationships)
  - `CodeConverterAgent` (guides translation prompts)
- **Interactions:**
  - Uses Azure OpenAI via Semantic Kernel with concurrency guard (e.g., 3 AI calls at a time).
  - Results persisted by `SqliteMigrationRepository`.

#### BusinessLogicExtractorAgent
- **Purpose:** Convert technical analyses into business language (use cases, user stories, glossary).
- **Inputs:** Output from `CobolAnalyzerAgent` + optional glossary.
- **Outputs:** `BusinessLogic` records and Markdown sections used in `reverse-engineering-details.md`.
- **Interactions:**
  - Runs in parallel with analyzer results.
  - Writes documentation via `FileHelper` and logs via `EnhancedLogger`.

#### DependencyMapperAgent
- **Purpose:** Identify CALL/COPY/PERFORM/IO relationships and build graph metadata.
- **Inputs:** COBOL files + analyses (line numbers, paragraphs).
- **Outputs:** `DependencyMap` with nodes/edges stored in both SQLite and Neo4j.
- **Interactions:**
  - Feeds the McpChatWeb graph panel and run-selector APIs.
  - Enables multi-run queries (e.g., "show me CALL tree for run 42").

#### CodeConverterAgent(s)
- **Variants:** `JavaConverterAgent` or `CSharpConverterAgent` (selected via `TargetLanguage`).
- **Purpose:** Generate target-language code from COBOL analyses and dependency context.
- **Inputs:**
  - `CobolAnalysis` per file
  - Target language settings (Quarkus vs. .NET)
  - Migration run metadata (for logging & metrics)
- **Outputs:** `CodeFile` records saved under `output/java-output/` or `output/dotnet-output/`.
- **Interactions:**
  - Concurrency guards (pipeline slots vs. AI calls) ensure Azure OpenAI limits respected.
  - Results pushed to portal via repositories for browsing/download.

### âš¡ Concurrency Notes
- **Pipeline concurrency (`--max-parallel`)** controls how many files/chunks run simultaneously (e.g., 8).
- **AI concurrency (`--max-ai-parallel`)** caps concurrent Azure OpenAI calls (e.g., 3) to avoid throttling.
- Both values can be surfaced via CLI flags or environment variables to let `doctor.sh` tune runtime.

### ğŸ”„ End-to-End Data Flow
1. `doctor.sh run` â†’ load configs â†’ choose target language â†’ optional reverse engineering skip.
2. `ReverseEngineeringProcess` â†’ discover files â†’ analyze â†’ extract business logic â†’ emit markdown/glossary.
3. `MigrationProcess` â†’ analyze (reuse or fresh) â†’ map dependencies â†’ convert code â†’ persist outputs.
4. `HybridMigrationRepository` coordinates writes to SQLite (structured data) and Neo4j (graph edges).
5. `McpServer` exposes data via MCP resources; `McpChatWeb` surfaces chat, graphs, reports.
6. Portal and MCP clients display progress, allow queries, and fetch generated artifacts.

### ğŸ” Configure Azure OpenAI Credentials

The project requires Azure OpenAI for **two purposes**:

1. **Migration Agents** (CobolAnalyzer, JavaConverter, DependencyMapper) - For code analysis and conversion
2. **MCP Chat Server** - For natural language queries about migration data

**Both use the same Azure OpenAI configuration** from `Config/appsettings.json`.

The project uses a secure two-file configuration system:

1. **`Config/ai-config.env`** - Template with default values (âœ… safe to commit)
2. **`Config/ai-config.local.env`** - Your actual credentials (âŒ never commit)

**Setup your credentials:**

```bash
# 1. Copy the template to create your local config
cp Config/ai-config.local.env.example Config/ai-config.local.env

# 2. Edit your local config with real values
nano Config/ai-config.local.env
```

**In `Config/ai-config.local.env`, update these lines:**
```bash
# Replace with your actual Azure OpenAI endpoint
AZURE_OPENAI_ENDPOINT="https://YOUR-RESOURCE-NAME.openai.azure.com/"

# Replace with your actual API key  
AZURE_OPENAI_API_KEY="your-32-character-api-key-here"

# Update deployment name to match your Azure setup
# Mini is a cheaper version and typically faster when testing
AZURE_OPENAI_DEPLOYMENT_NAME="gpt-5-mini-2"
```

**ğŸ” How to find your Azure OpenAI values:**
- **Endpoint**: Azure Portal â†’ Your OpenAI Resource â†’ "Resource Management" â†’ "Keys and Endpoint" â†’ Endpoint
- **API Key**: Azure Portal â†’ Your OpenAI Resource â†’ "Resource Management" â†’ "Keys and Endpoint" â†’ Key 1
- **Deployment Name**: Azure AI Foundry â†’ Your deployment name (e.g., "gpt-5-mini-2" or "gpt-4o")

**ğŸ“‹ Example `ai-config.local.env` with real values:**
```bash
# Example - replace with your actual values
AZURE_OPENAI_ENDPOINT="https://my-company-openai.openai.azure.com/"
AZURE_OPENAI_API_KEY="1234567890abcdef1234567890abcdef"
AZURE_OPENAI_DEPLOYMENT_NAME="gpt-5-mini-2"
AZURE_OPENAI_MODEL_ID="gpt-5-mini"
```

**âš ï¸ IMPORTANT**: 
- Make sure your endpoint ends with `/`
- API key should be 32 characters long
- Deployment name must match your Azure OpenAI deployment (e.g., "gpt-5-mini-2", "gpt-4o")

**Usage:** Same credentials power both migration agents and MCP chat server via `Config/appsettings.json`

### Neo4j Database

**Quick Start:**
```bash
# Auto-starts in dev container (recommended)
# OR manual start:
docker-compose up -d neo4j && docker ps | grep neo4j
```

**Connection Details:**
- **HTTP (Browser)**: http://localhost:7474
- **Bolt (Driver)**: bolt://localhost:7687
- **Username**: `neo4j`
- **Password**: `cobol-migration-2025`

---

## ğŸ¯ CLI Reference

### Doctor.sh - Three Migration Modes

The `doctor.sh` script provides three distinct migration workflows:

#### 1ï¸âƒ£ **Reverse Engineering Only** (No UI)
```bash
./doctor.sh reverse-eng
```
**What it does:**
- Extracts business logic, feature descriptions, and use cases from COBOL
- Generates `output/reverse-engineering-details.md` documentation
- **Does NOT** convert code to Java or C#
- **Does NOT** launch web UI
- Use when: You only need business documentation

#### 2ï¸âƒ£ **Full Migration** (Reverse Eng + Conversion + UI + Report)
```bash
./doctor.sh run
```
**What it does:**
- First runs reverse engineering (or uses existing results)
- Then converts COBOL to Java Quarkus (or C# .NET - you choose)
- Generates all reports and documentation
- **Prompts to generate migration report** with comprehensive details
- **Launches web UI** at http://localhost:5028
- Use when: You want complete migration with business documentation

**New Features:**
- âœ… Target language selection (Java Quarkus or C# .NET)
- âœ… Optional report generation after migration
- âœ… Enhanced dependency tracking (CALL, COPY, PERFORM, EXEC, READ, WRITE, OPEN, CLOSE)
- âœ… Line-level context for all dependencies

#### 3ï¸âƒ£ **Conversion Only** (Skip Reverse Eng, Launch UI)
```bash
./doctor.sh convert-only
```
**What it does:**
- Skips reverse engineering entirely
- Only performs COBOL to Java Quarkus or C# .NET conversion (you choose)
- Generates Java/C# code and migration reports
- **âœ… Launches web UI** at http://localhost:5250
- Use when: You already have documentation or just need code conversion

### Other Doctor.sh Commands
```bash
./doctor.sh                    # Diagnose configuration
./doctor.sh setup              # Interactive setup wizard
./doctor.sh test               # System validation and health check
```

**Helper Scripts:**
```bash
./helper-scripts/demo.sh                    # Quick demo with sample COBOL files
./helper-scripts/verify-data-persistence.sh # Check database integrity
./helper-scripts/verify-port-standardization.sh # Verify ports 5028, 7474, 7687
```

### Direct .NET Commands
```bash
# Full migration with reverse engineering
dotnet run -- --source ./source

# Skip reverse engineering
dotnet run -- --source ./source --skip-reverse-engineering

# Reverse engineering only
dotnet run reverse-engineer --source ./source

# MCP server standalone
dotnet run mcp [--run-id 42]

# Conversation log viewer
dotnet run conversation
```

**Options:** `--verbose`, `--config <path>`, `--run-id <number>`, `--session-id <id>`

## Step-by-Step Guide

1. **Configure:** `cp Config/ai-config.local.env.example Config/ai-config.local.env` â†’ Add Azure OpenAI endpoint, API key, deployment name
2. **Add COBOL files:** Place your COBOL files in `./source/`
3. **Run:** `./doctor.sh run` - Analyzes, converts (choose Java or C#), launches portal at http://localhost:5028
4. **Choose target:** Select Java Quarkus or C# .NET when prompted
5. **Explore:** Use portal UI (chat, graph) or connect MCP clients (Claude, Cursor)
6. **Results:** Generated code in `output/`, documentation in `output/`, logs in `Logs/`

**Portal features:** Three-panel UI, AI chat, multi-run queries, interactive graph, file content analysis

**Env variables:** `MCP_AUTO_LAUNCH=0` (skip portal), `MIGRATION_DB_PATH` (custom DB location)

---

## ğŸ§ª Testing & Validation

```bash
# Run integration tests
dotnet test McpChatWeb.Tests/McpChatWeb.Tests.csproj

# Validate configuration
./doctor.sh test

# Verify data persistence
./helper-scripts/verify-data-persistence.sh

# Check port standardization
./helper-scripts/verify-port-standardization.sh
```

---

## ğŸ“Š Real Migration Stats

- **102 COBOL files** processed â†’ **99 Java Quarkus files** generated (97% success rate)
- **205 Azure OpenAI API calls**, ~1.2 hours total, $0.31 cost
- **Outputs**: Java/C# code in `output/`, docs in `output/`, logs in `Logs/`, metadata in `Data/migration.db`, graph in Neo4j

---

## â˜ï¸ Azure Container Apps Deployment

Deploy this demo to Azure Container Apps with a single command using Azure Developer CLI (azd).

### Prerequisites

- Azure CLI installed (`az --version`)
- Azure Developer CLI installed (`azd version`) - [Install azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd)
- Azure subscription with permissions to create resources
- `Config/ai-config.local.env` configured with your Azure OpenAI credentials

### Quick Deploy

```bash
# 1. Login to Azure (both azd and az CLI are required)
azd auth login
az account show >/dev/null 2>&1 || az login

# 2. Create a new environment
azd env new cobol-demo-prod

# 3. (Optional) Specify Azure subscription and location
azd env set AZURE_SUBSCRIPTION_ID "your-subscription-id"
azd env set AZURE_LOCATION "japaneast"

# 4. Deploy everything in one command!
azd up

```
> **Tip:** If missing Data/migration.db, run a migration first. Use `doctor.sh run`

That's it! The deployment automatically reads your Azure OpenAI settings from `Config/ai-config.local.env`.

### What Gets Deployed

| Resource | Description |
|----------|-------------|
| **Resource Group** | `rg-<env-name>` |
| **Container Apps Environment** | Managed environment for containers |
| **Container Registry** | Private registry for app images |
| **Container App (mcpchatweb)** | Web portal (externally accessible) |
| **Container App (neo4j)** | Graph database (internal only) |
| **Storage Account** | Azure Files for data persistence |
| **Log Analytics** | Centralized logging |

### Resource Group Naming

The resource group name is determined by the environment name you specify:

```
rg-${AZURE_ENV_NAME}
```

**Examples:**

| azd Environment Name | Resource Group Name |
|---------------------|---------------------|
| `cobol-demo-prod` | `rg-cobol-demo-prod` |

### Environment Variables Reference

Environment variables are automatically loaded from `Config/ai-config.local.env`. You can also override them with `azd env set`.

| Variable | Required | Description |
|----------|----------|-------------|
| `AZURE_OPENAI_ENDPOINT` | âœ… | Azure OpenAI endpoint URL |
| `AZURE_OPENAI_API_KEY` | âœ… | Azure OpenAI API key |
| `AZURE_OPENAI_DEPLOYMENT_NAME` | âœ… | Model deployment name |
| `AZURE_OPENAI_MODEL_ID` | âŒ | Model ID (default: from config) |
| `NEO4J_PASSWORD` | âŒ | Neo4j password (default: `cobol-migration-2025`) |

### After Deployment

```bash
# View deployed resources
azd show

# Get the Web Portal URL
azd env get-values | grep SERVICE_MCPCHATWEB_URI

# View logs
az containerapp logs show --name mcpchatweb --resource-group rg-<env-name>

# Tear down all resources
azd down

```

### Update Deployment

```bash
# Deploy code changes only
azd deploy

# Update infrastructure and code
azd up

```

---

## ğŸ”§ Advanced Topics
- **Folder structure:** Input: `source/`, Output: `output/`
- **Token limits:** GPT-5 Mini supports 32K tokens - adjust per agent in `appsettings.json`  
- **Agent personas:** Modify prompts for different outcomes (DB2â†’PostgreSQL, other languages)
- **Legacy languages:** Framework adaptable to APL, PL/I - update agent prompts accordingly

**Known Issues:**
- Content filtering may block Azure OpenAI calls
- Token limits: Don't exceed MaxTokens settings (32K for GPT-5 Mini)
- Model deployment names must match Azure configuration

**Extension Ideas:**
- Enhance agent prompts for specific migration scenarios
- Add support for additional legacy languages (PL/I, RPG, APL)
- Integrate with CI/CD pipelines  
- Improve test generation capabilities
- Contributions welcome!

---

## Disclaimer

This software is provided for **demonstration purposes only**. It is not intended to be relied upon for production use. The creators make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability. Any reliance on this software is strictly at your own risk.

## License

MIT License - Copyright (c) Microsoft Corporation. See full license terms in the repository.

---

## ğŸ“ Known TODOs

### Project Naming Cleanup
- [ ] **Rename project from "CobolToQuarkusMigration" to "LegacyModernizationAgents"**
  - Update `CobolToQuarkusMigration.csproj` â†’ `LegacyModernizationAgents.csproj`
  - Update references in `Legacy-Modernization-Agents.sln`
  - Update `doctor.sh` script references
  - Update namespace declarations in C# files
  - Update documentation references
  - Update Docker and build configuration files
  - Rationale: Project now supports multiple target languages (Java Quarkus and C# .NET), making the original name too specific

### Code Refactoring
- [ ] **Refactor parameter names in `MigrationProcess.cs`**
  - Rename `javaOutputFolder` â†’ `outputFolder`
  - Rename `cobolSourceFolder` â†’ `sourceFolder`
  - Rationale: Current names are language-specific but the framework supports multiple target languages (Java and C#)

````
